# Verification

Software verification is the process of evaluating a product against it’s required specifications. This involves reviewing code and design decisions throughout the development process to ensure accuracy in development a reliable solution to the given specifications. Testing is also a crucial aspect, giving us confidence that our code works as expected.   

Developing a product of any kind is very costly, both in money and time. As humans, it is inevitable that mistakes are oversights will appear as we create our program. These mistakes can be variable in severity, but even the smallest mistakes can become major setbacks later along the line. Through software verification, we can minimise the chance of missing errors or incorrect design solutions that would not satisfy the program specifications. It is also important that our final product is reliable and satisfies what the customer initially wanted.

We were able to implement software verification in iteration 2 through code reviews. Whenever a user completed a section of code and pushed, we would have another people take a look at their code. By having two people look through code before pushing we minimises the number of bugs in pushed code. Helping maintain a functioning product throughout development. When iteration 2 was first released, we got together as a team and discussed and planned. We decided to develop our program in order of dependency. This means as we wrote functions, we could always test them. Frequent testing ensured that we dealt with bugs as they appeared and we wouldn’t be overwhelmed at the end. Peer-programming in person or through discord screen-share allowed for a significant decrease in mistakes in coding. The person watching could also identify better methods. 

We used dynamic verification through writing pytest tests. Our pytests incorporated unit and integration testing. Unit testing is the testing of an individual software components, in our case an individual function called by the flask server. Through the use of pytest, we could compare the output of the function with the expected output as described in the specifications. We would check the correct value was return and if errors were raised in the case of invalid input. This allowed us to ensure these basic functions (such as auth_register) were functioning. Once we had these basic functions written and tested, we could use integration testing to ensure functions were correctly interacting with the data and with the other functions. We were able to verify a function by creating potential cases the function would be called. We would register users using auth_register, create a channel using channel_create, send a message using message_send, message_react to react to the message and use channel_detail to check the message got reacted to. By doing this, we can test that each part of the chain is working as if any one of them were unsuccessful, all next functions would also fail. By creating these chains, we ensure that not only do the functions work by themselves in isolation, but will also work in the system as a whole.  

# Validation
Software validation is the process of ensuring that a system is successful in accomplishing its intended use and satisfies the end user’s needs and requirements. 

While software verification is required to confirm our program works properly software validation is ensuring this correctly working software actually is working to achieve what we initially set out to accomplish. 

A key part of software validation is having a clear understanding on what the software’s end goal is. This is done by developing a thorough collection of user stories and acceptance criteria. These tell us what we have to achieve for our system to be “right”. By developing our program according to these criteria, we can ensure our program is what the user wants. To verify we achieved these criteria we can use acceptance testing. Acceptance testing is where users test the program and determine if it is acceptable in regards to the user’s needs.    

Some tools we used for assurance were:
* Code coverage
* Pylint
* Pytest

Pytest allowed us to write unit and integrated tests for our functions. These tests could check that the correct value was returned and the correct error was raised. We used pytest along side code coverage. Code coverage allowed us to get a report regarding how much of the function code was actually being run by our tests and thus how much were our tests testing. While the code coverage score is not a linear relationship where 100% is twice as good as 50%, having a high code coverage indicates we have tested a large range of specific and edge cases. This means we can be more confident our program will work in every case. While it is not always necessary to have a code coverage of 100, code coverage allows us to see which branches or code are not being run in the tests. This is important as it allows us to determine if we have missed testing any especially important or essential code. It enabled us to expand our tests to cases we did not consider or forgot. We also used the linter Pylint. A linter is a program that analyses code to look for errors, bugs and syntax errors. By using a linter for all our files, we can ensure that we are abiding by good programming practices. As multiple people are working on code, it also ensures there is uniformity throughout the different files. As people abide by syntax rules, functions are easily understood by members that weren’t familiar with them.    

